import os
import torch
import torch.nn.functional as F
import numpy as np
from pathlib import Path
import json
from datetime import datetime
import argparse
from PIL import Image
import torchvision.transforms as transforms
from sklearn.metrics.pairwise import cosine_similarity
import warnings
import re

# ê²½ê³  ë©”ì‹œì§€ ì–µì œ
warnings.filterwarnings('ignore')

class VideoFrameDeduplicator:
    def __init__(self, folder_path, similarity_threshold=0.95, model_name='dino_vits16', 
                 device=None, window_size=5):
        """
        ë¹„ë””ì˜¤ í”„ë ˆì„ ì¤‘ë³µ ì œê±° í´ë˜ìŠ¤ (ì¸ì ‘ í”„ë ˆì„ë§Œ ë¹„êµ)
        
        Args:
            folder_path (str): ì´ë¯¸ì§€ê°€ ìˆëŠ” í´ë” ê²½ë¡œ
            similarity_threshold (float): ìœ ì‚¬ë„ ì„ê³„ê°’ (0.0-1.0, ë†’ì„ìˆ˜ë¡ ì—„ê²©)
            model_name (str): DINO ëª¨ë¸ ì´ë¦„
            device (str): ë””ë°”ì´ìŠ¤ ('cuda', 'cpu', None=ìë™ì„ íƒ)
            window_size (int): ë¹„êµí•  ì¸ì ‘ í”„ë ˆì„ ìœˆë„ìš° í¬ê¸° (ê¸°ë³¸ê°’: 5)
        """
        self.folder_path = Path(folder_path)
        self.similarity_threshold = similarity_threshold
        self.model_name = model_name
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
        self.window_size = window_size
        self.image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif', '.webp'}
        
        print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        print(f"DINO ëª¨ë¸: {model_name}")
        print(f"ì¸ì ‘ í”„ë ˆì„ ìœˆë„ìš° í¬ê¸°: {window_size}")
        
        # DINO ëª¨ë¸ ë¡œë“œ
        self.model = self._load_dino_model()
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë³€í™˜
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
    def _load_dino_model(self):
        """DINO ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            print("DINO ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” ì¤‘...")
            model = torch.hub.load('facebookresearch/dino:main', self.model_name)
            model = model.to(self.device)
            model.eval()
            print("DINO ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
            return model
        except Exception as e:
            print(f"DINO ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
            raise
    
    def extract_frame_number(self, filename):
        """íŒŒì¼ëª…ì—ì„œ í”„ë ˆì„ ë²ˆí˜¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        # ì¼ë°˜ì ì¸ íŒ¨í„´ë“¤: frame_001.jpg, img_123.png, 001.jpg, video_001_frame_456.jpg ë“±
        patterns = [
            r'frame_(\d+)',
            r'img_(\d+)', 
            r'image_(\d+)',
            r'(\d+)(?=\.[^.]*$)',  # í™•ì¥ì ì§ì „ì˜ ìˆ«ì
            r'_(\d+)_',
            r'_(\d+)$'
        ]
        
        filename_lower = filename.lower()
        for pattern in patterns:
            matches = re.findall(pattern, filename_lower)
            if matches:
                return int(matches[-1])  # ë§ˆì§€ë§‰ ë§¤ì¹˜ëœ ë²ˆí˜¸ ì‚¬ìš©
        
        return 0  # ë²ˆí˜¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ 0 ë°˜í™˜
    
    def get_image_files(self):
        """í´ë”ì—ì„œ ì´ë¯¸ì§€ íŒŒì¼ ëª©ë¡ì„ ê°€ì ¸ì™€ì„œ í”„ë ˆì„ ìˆœì„œëŒ€ë¡œ ì •ë ¬í•©ë‹ˆë‹¤."""
        image_files = []
        for ext in self.image_extensions:
            image_files.extend(self.folder_path.glob(f'*{ext}'))
            #image_files.extend(self.folder_path.glob(f'*{ext.upper()}'))
        
        # í”„ë ˆì„ ë²ˆí˜¸ë¡œ ì •ë ¬ (ë²ˆí˜¸ê°€ ê°™ìœ¼ë©´ íŒŒì¼ëª…ìœ¼ë¡œ ì •ë ¬)
        def sort_key(path):
            frame_num = self.extract_frame_number(path.stem)
            return (frame_num, path.name)
        
        sorted_files = sorted(image_files, key=sort_key)
        
        # ì •ë ¬ ê²°ê³¼ í™•ì¸ìš© ì¶œë ¥ (ì²˜ìŒ ëª‡ ê°œë§Œ)
        #for i, file_path in enumerate(sorted_files[:5]) :
        #    frame_num = self.extract_frame_number(file_path.stem)
        #    print(f"  {i+1}. {file_path.name} (frame: {frame_num})")
         
        return sorted_files
    
    def extract_features(self, image_path):
        """DINO ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ íŠ¹ì„±ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            image = Image.open(image_path).convert('RGB')
            image_tensor = self.transform(image).unsqueeze(0).to(self.device)
            
            with torch.no_grad():
                features = self.model(image_tensor)
            
            features = features.cpu().numpy().flatten()
            features = features / np.linalg.norm(features)
            
            return features
            
        except Exception as e:
            print(f"íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨ {image_path}: {e}")
            return None
    
    def calculate_similarity(self, features1, features2):
        """ë‘ íŠ¹ì„± ë²¡í„° ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        if features1 is None or features2 is None:
            return 0.0
        
        similarity = cosine_similarity([features1], [features2])[0][0]
        similarity = (similarity + 1) / 2  # 0~1ë¡œ ì •ê·œí™”
        
        return float(similarity)
    
    def get_image_info(self, image_path):
        """ì´ë¯¸ì§€ì˜ ê¸°ë³¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            with Image.open(image_path) as img:
                width, height = img.size
                file_size = image_path.stat().st_size
                
                return {
                    'width': width,
                    'height': height,
                    'file_size': file_size,
                    'resolution': width * height
                }
        except Exception as e:
            print(f"ì´ë¯¸ì§€ ì •ë³´ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ {image_path}: {e}")
            return None
    
    def choose_better_image(self, img1_path, img2_path):
        """ë‘ ìœ ì‚¬í•œ ì´ë¯¸ì§€ ì¤‘ ë” ë‚˜ì€ ì´ë¯¸ì§€ë¥¼ ì„ íƒí•©ë‹ˆë‹¤."""
        info1 = self.get_image_info(img1_path)
        info2 = self.get_image_info(img2_path)
        
        if info1 is None:
            return img2_path
        if info2 is None:
            return img1_path
        
        # í•´ìƒë„ê°€ ë†’ì€ ê²ƒì„ ìš°ì„ 
        if info1['resolution'] != info2['resolution']:
            return img1_path if info1['resolution'] > info2['resolution'] else img2_path
        
        # íŒŒì¼ í¬ê¸°ê°€ í° ê²ƒì„ ìš°ì„ 
        return img1_path if info1['file_size'] > info2['file_size'] else img2_path
    
    def process_images_sequential(self):
        """ì¸ì ‘í•œ í”„ë ˆì„ë“¤ë§Œ ë¹„êµí•˜ì—¬ ì¤‘ë³µì„ ì œê±°í•©ë‹ˆë‹¤."""
        image_files = self.get_image_files()
        
        if not image_files:
            print("ì²˜ë¦¬í•  ì´ë¯¸ì§€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return [], []
        
        if len(image_files) < 2:
            print("ë¹„êµí•  ì´ë¯¸ì§€ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return image_files, []
        
        print(f"ì´ {len(image_files)}ê°œì˜ ë¹„ë””ì˜¤ í”„ë ˆì„ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
        print("ì¸ì ‘ í”„ë ˆì„ ë¹„êµë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        unique_images = []
        duplicate_groups = []
        comparison_count = 0
        
        i = 0
        while i < len(image_files):
            print(f"\ní”„ë ˆì„ {i+1} ({image_files[i].name}) ì²˜ë¦¬ ì¤‘...")
            
            # í˜„ì¬ í”„ë ˆì„ (në²ˆì§¸) íŠ¹ì„± ì¶”ì¶œ
            current_image = image_files[i]
            current_features = self.extract_features(current_image)
            
            # ì—°ì†ëœ ìœ ì‚¬í•œ í”„ë ˆì„ë“¤ ì°¾ê¸°
            consecutive_duplicates = []
            j = i + 1
            
            # n+1, n+2, n+3... ìˆœì°¨ì ìœ¼ë¡œ ë¹„êµ
            while j < len(image_files):
                next_image = image_files[j]
                next_features = self.extract_features(next_image)
                
                # ë¹„êµ íšŸìˆ˜ ì¦ê°€
                comparison_count += 1
                
                # í˜„ì¬ í”„ë ˆì„(n)ê³¼ ë‹¤ìŒ í”„ë ˆì„(n+k) ë¹„êµ
                similarity = self.calculate_similarity(current_features, next_features)
                
                print(f"  â†’ í”„ë ˆì„ {j+1} ë¹„êµ: {similarity:.4f}")
                
                if similarity >= self.similarity_threshold:
                    # ìœ ì‚¬í•œ í”„ë ˆì„ ë°œê²¬ - ì¤‘ë³µ ëª©ë¡ì— ì¶”ê°€
                    consecutive_duplicates.append(next_image)
                    j += 1
                else:
                    break
            
            # í˜„ì¬ í”„ë ˆì„ì„ ëŒ€í‘œë¡œ ì„ íƒ (í’ˆì§ˆ ì²´í¬ í›„)
            if consecutive_duplicates:
                # ì¤‘ë³µ ê·¸ë£¹ì—ì„œ ìµœê³  í’ˆì§ˆ ì°¾ê¸°
                all_in_group = [current_image] + consecutive_duplicates
                best_image = current_image
                
                for img in consecutive_duplicates:
                    if self.choose_better_image(best_image, img) != best_image:
                        best_image = img
                
                unique_images.append(best_image)
                
                print(f"ê·¸ë£¹ ëŒ€í‘œ ì´ë¯¸ì§€ : {best_image.name} (ì´ {len(all_in_group)}ê°œ í”„ë ˆì„ ê·¸ë£¹)")
                
                # ì¤‘ë³µ ê·¸ë£¹ ì •ë³´ ì €ì¥ (ìœ ì‚¬ë„ëŠ” ê°„ì†Œí™”)
                duplicate_groups.append({
                    'selected': str(best_image),
                    'duplicates': [str(img) for img in all_in_group if img != best_image],
                    'group_size': len(all_in_group),
                    'frame_range': f"{self.extract_frame_number(all_in_group[0].stem)}-"
                                 f"{self.extract_frame_number(all_in_group[-1].stem)}"
                })
            else:
                # ì¤‘ë³µì´ ì—†ëŠ” ë…ë¦½ í”„ë ˆì„
                unique_images.append(current_image)
                print(f"ë…ë¦½ í”„ë ˆì„: {current_image.name}")
            
            # ë‹¤ìŒ ì²˜ë¦¬í•  í”„ë ˆì„ìœ¼ë¡œ ì´ë™ (ì¤‘ë³µëœ í”„ë ˆì„ë“¤ì€ ëª¨ë‘ ê±´ë„ˆë›°ê¸°)
            if j < len(image_files):
                i = j  # ìœ ì‚¬í•˜ì§€ ì•Šì€ í”„ë ˆì„ë¶€í„° ë‹¤ì‹œ ì‹œì‘
                #print(f"ë‹¤ìŒ ì‹œì‘ì : í”„ë ˆì„ {i+1}")
            else:
                i = len(image_files)  # ëª¨ë“  í”„ë ˆì„ ì²˜ë¦¬ ì™„ë£Œ
        
        print(f"\nì²˜ë¦¬ ì™„ë£Œ! ì´ ë¹„êµ íšŸìˆ˜: {comparison_count}ë²ˆ")
        print(f"   ì›ë³¸ í”„ë ˆì„: {len(image_files)}ê°œ")
        print(f"   ê³ ìœ  í”„ë ˆì„: {len(unique_images)}ê°œ")
        print(f"   ì œê±°ëœ í”„ë ˆì„: {len(image_files) - len(unique_images)}ê°œ")
        
        return unique_images, duplicate_groups
    
    def save_results(self, unique_images, duplicate_groups):
        """ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # 1. ê³ ìœ  ì´ë¯¸ì§€ ëª©ë¡ì„ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥
        unique_list_path = self.folder_path / f"unique_frames_dino.txt"
        with open(unique_list_path, 'w', encoding='utf-8') as f:
            #f.write(f"# DINO ê¸°ë°˜ ë¹„ë””ì˜¤ í”„ë ˆì„ ì¤‘ë³µ ì œê±° í›„ ê³ ìœ  ì´ë¯¸ì§€ ëª©ë¡\n")
            #f.write(f"# ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            #f.write(f"# ëª¨ë¸: {self.model_name}\n")
            #f.write(f"# ìœ ì‚¬ë„ ì„ê³„ê°’: {self.similarity_threshold}\n")
            #f.write(f"# ì¸ì ‘ í”„ë ˆì„ ìœˆë„ìš° í¬ê¸°: {self.window_size}\n")
            #f.write(f"# ì´ {len(unique_images)}ê°œ í”„ë ˆì„\n\n")
            
            for img_path in unique_images:
                frame_num = self.extract_frame_number(img_path.stem)
                #f.write(f"{img_path.name}\t# frame: {frame_num}\n")
                f.write(f"{img_path.name}\n")
        
        # 2. ì¤‘ë³µ ì •ë³´ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        duplicate_info_path = self.folder_path / f"duplicate_frames_info.json"
        
        total_original_frames = len(unique_images) + sum(group['group_size'] - 1 for group in duplicate_groups)
        compression_ratio = len(unique_images) / total_original_frames if total_original_frames > 0 else 1.0
        
        duplicate_info = {
            'summary': {
                'total_unique_frames': len(unique_images),
                'total_original_frames': total_original_frames,
                'total_duplicate_groups': len(duplicate_groups),
                'total_duplicates_removed': sum(group['group_size'] - 1 for group in duplicate_groups),
                'compression_ratio': compression_ratio,
                'model_name': self.model_name,
                'similarity_threshold': self.similarity_threshold,
                'window_size': self.window_size,
                'device_used': self.device,
                'processed_at': datetime.now().isoformat()
            },
            'duplicate_groups': duplicate_groups
        }
        
        with open(duplicate_info_path, 'w', encoding='utf-8') as f:
            json.dump(duplicate_info, f, ensure_ascii=False, indent=2)
        
        # 3. ìƒì„¸ ë³´ê³ ì„œ ìƒì„±
        report_path = self.folder_path / f"frame_deduplication_report.txt"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("DINO ê¸°ë°˜ ë¹„ë””ì˜¤ í”„ë ˆì„ ì¤‘ë³µ ì œê±° ë³´ê³ ì„œ\n")
            f.write("=" * 60 + "\n\n")
            f.write(f"ì²˜ë¦¬ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"í´ë” ê²½ë¡œ: {self.folder_path}\n")
            f.write(f"DINO ëª¨ë¸: {self.model_name}\n")
            f.write(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}\n")
            f.write(f"ìœ ì‚¬ë„ ì„ê³„ê°’: {self.similarity_threshold}\n")
            f.write(f"ì²˜ë¦¬ ë°©ì‹: ì¸ì ‘ í”„ë ˆì„ ìˆœì°¨ ë¹„êµ\n\n")
            
            f.write("ìš”ì•½\n")
            f.write("-" * 20 + "\n")
            f.write(f"ì›ë³¸ í”„ë ˆì„ ìˆ˜: {total_original_frames}\n")
            f.write(f"ê³ ìœ  í”„ë ˆì„ ìˆ˜: {len(unique_images)}\n")
            f.write(f"ì¤‘ë³µ ê·¸ë£¹ ìˆ˜: {len(duplicate_groups)}\n")
            f.write(f"ì œê±°ëœ ì¤‘ë³µ í”„ë ˆì„ ìˆ˜: {sum(group['group_size'] - 1 for group in duplicate_groups)}\n")
            f.write(f"ì••ì¶•ìœ¨: {compression_ratio:.2%}\n\n")
            
            if duplicate_groups:
                f.write("ì¤‘ë³µ í”„ë ˆì„ ê·¸ë£¹ ìƒì„¸ ì •ë³´\n")
                f.write("-" * 40 + "\n")
                for i, group in enumerate(duplicate_groups, 1):
                    f.write(f"\nê·¸ë£¹ {i} (ì—°ì† í”„ë ˆì„ {group['frame_range']}, ì´ {group['group_size']}ê°œ):\n")
                    f.write(f"  ì„ íƒëœ í”„ë ˆì„: {group['selected']}\n")
                    f.write(f"  ì œê±°ëœ í”„ë ˆì„ë“¤:\n")
                            
        return unique_list_path, duplicate_info_path, report_path

def main():
    parser = argparse.ArgumentParser(description='ë¹„ë””ì˜¤ í”„ë ˆì„ ì¤‘ë³µ ì œê±° ë„êµ¬ (ì¸ì ‘ í”„ë ˆì„ ë¹„êµ)')
    parser.add_argument('folder_path', help='ë¹„ë””ì˜¤ í”„ë ˆì„ ì´ë¯¸ì§€ê°€ ìˆëŠ” í´ë” ê²½ë¡œ')
    parser.add_argument('--threshold', type=float, default=0.75,
                        help='ìœ ì‚¬ë„ ì„ê³„ê°’ (0.0-1.0, ê¸°ë³¸ê°’: 0.75)')
    parser.add_argument('--model', type=str, default='dino_vits16',
                        choices=['dino_vits16', 'dino_vits8', 'dino_vitb16', 'dino_vitb8'],
                        help='DINO ëª¨ë¸ ì„ íƒ (ê¸°ë³¸ê°’: dino_vits16)')
    parser.add_argument('--device', type=str, default=None,
                        choices=['cuda', 'cpu'],
                        help='ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ (ê¸°ë³¸ê°’: ìë™ì„ íƒ)')
    parser.add_argument('--window-size', type=int, default=5,
                        help='ì¸ì ‘ í”„ë ˆì„ ìœˆë„ìš° í¬ê¸° (ê¸°ë³¸ê°’: 5)')
    
    args = parser.parse_args()
    
    # í´ë” ì¡´ì¬ í™•ì¸
    if not os.path.exists(args.folder_path):
        print(f"ì˜¤ë¥˜: í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.folder_path}")
        return
    
    try:
        # ì¤‘ë³µ ì œê±° ì‹¤í–‰
        deduplicator = VideoFrameDeduplicator(
            args.folder_path, 
            args.threshold, 
            args.model,
            args.device,
            args.window_size
        )
        
        unique_images, duplicate_groups = deduplicator.process_images_sequential()
        
        if unique_images:
            # ê²°ê³¼ ì €ì¥
            unique_list_path, duplicate_info_path, report_path = deduplicator.save_results(
                unique_images, duplicate_groups
            )
            
            total_original = len(unique_images) + sum(group['group_size'] - 1 for group in duplicate_groups)
            compression_ratio = len(unique_images) / total_original if total_original > 0 else 1.0
            
            print(f"\nğŸ¬ ë¹„ë””ì˜¤ í”„ë ˆì„ ì¤‘ë³µ ì œê±° ì™„ë£Œ!")
            print(f"ì›ë³¸ í”„ë ˆì„ ìˆ˜: {total_original}")
            print(f"ê³ ìœ  í”„ë ˆì„ ìˆ˜: {len(unique_images)}")
            print(f"ì œê±°ëœ ì¤‘ë³µ í”„ë ˆì„ ìˆ˜: {sum(group['group_size'] - 1 for group in duplicate_groups)}")
            print(f"ì••ì¶•ìœ¨: {compression_ratio:.2%}")
            print(f"ë¹„êµ íšŸìˆ˜: ì•½ {len(unique_images) + sum(group['group_size'] - 1 for group in duplicate_groups) - 1}ë²ˆ (ìˆœì°¨ ë¹„êµ)")
            print(f"\nìƒì„±ëœ íŒŒì¼:")
            print(f"  - ê³ ìœ  í”„ë ˆì„ ëª©ë¡: {unique_list_path}")
            print(f"  - ì¤‘ë³µ ì •ë³´: {duplicate_info_path}")
            print(f"  - ìƒì„¸ ë³´ê³ ì„œ: {report_path}")
        else:
            print("ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        print(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        print("í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    main()